{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc52fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a08f54",
   "metadata": {},
   "source": [
    "### 3 -  Loading Different Document Formats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa0ad5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ffe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install chromadb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0e86fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading PDF, DOCX and TXT files as LangChain Documents\n",
    "def load_document(file) -> List[str]:\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "    try:\n",
    "        if extension == '.pdf':\n",
    "            from langchain.document_loaders import PyPDFLoader\n",
    "            print(f'Loading {file}')\n",
    "            loader = PyPDFLoader(file)\n",
    "        elif extension == '.docx':\n",
    "            from langchain.document_loaders import Docx2txtLoader\n",
    "            print(f'Loading {file}')\n",
    "            loader = Docx2txtLoader(file)\n",
    "        elif extension == '.txt':\n",
    "            from langchain.document_loaders import TextLoader\n",
    "            loader = TextLoader(file)\n",
    "        else:\n",
    "           # print('Document format is not supported!')\n",
    "           # return None\n",
    "            raise Exception ('Document format is not supported!')\n",
    "    except Exception as e:\n",
    "        print('Caught a document error: ' + repr(e)) \n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e337ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/attention_is_all_you_need.pdf\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiﬁcantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\n",
      "in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v5  [cs.CL]  6 Dec 2017\n"
     ]
    }
   ],
   "source": [
    "data = load_document('files/attention_is_all_you_need.pdf')\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_wikipedia(query:str, lang:str = 'en'):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=2)\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f09eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0) \n",
    "    chunks = text_splitter.split_documents(data) \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf64e722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.0004:.6f}')\n",
    "    \n",
    "    \n",
    "# print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb58fc0",
   "metadata": {},
   "source": [
    "#next step is to create embeddings and print them and then upload to a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9bc87f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create embeddings using OpenAIEmbeddings() and save them in a Chroma vector store\n",
    "def create_embeddings(chunks):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vector_store = Chroma.from_documents(chunks, embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name):\n",
    "    import pinecone\n",
    "    from langchain.vectorstores import Pinecone\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    pinecone.init(api_key=os.enviro.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "    \n",
    "    if index_name in pinecone.list_indexes():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ...', ends='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('ok')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "        pinecone.create_index(index_name, dimension=1536, metric='cosine')\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print('ok')\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0034fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    pinecone.init(api_key=os.enviro.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "    if index_name == 'all':\n",
    "        indexes = pinecone.list_indexes()\n",
    "        print('deleteing all indexes')\n",
    "        for index in indexes:\n",
    "            pinecone.delete_index(index)\n",
    "        print('ok')\n",
    "    else:\n",
    "        pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d777fffc",
   "metadata": {},
   "source": [
    "Use chains to combine LLM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae83265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q, k=3):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.2)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "    answer = chain.run(q)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c63da85",
   "metadata": {},
   "source": [
    "### RUNNING CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "db4952d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 pages in your data\n"
     ]
    }
   ],
   "source": [
    "data = load_document('files/state_of_the_union.txt')\n",
    "print (f'You have {len(data)} pages in your data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "16d960bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf55a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = create_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f90158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context, it appears that the document is about the formation of character, finding purpose, and forging the future of a nation. It emphasizes the commitment to protecting freedom, expanding fairness and opportunity, and saving democracy.\n"
     ]
    }
   ],
   "source": [
    "q = 'what is the whole document about?'\n",
    "# q = 'what is resoning and acting in LLMs?'\n",
    "# q = 'Summarize the entire document in a few paragraphs.'\n",
    "\n",
    "k = 3\n",
    "answer = ask_and_get_answer(vector_store, q, k)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44a2901f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Quit of Exit to quit.\n",
      "Question #1: What did the president say about Ketanji Brown Jackson?\n",
      "The president said that Ketanji Brown Jackson is one of our nation's top legal minds and that she will continue Justice Breyer's legacy of excellence.\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "Question #2: quit\n",
      "Quitting ... Bye Bye!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "i = 1\n",
    "print('Write Quit or Exit to quit.')\n",
    "while True:\n",
    "    q = input(f'Question #{i}: ')\n",
    "    i = i+1\n",
    "    if q.lower() in  ['quit', 'exit']:\n",
    "        print('Quitting ... Bye Bye!')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "        \n",
    "    answer = ask_and_get_answer(vector_store, q, 5)\n",
    "    print(answer)\n",
    "    print(f'\\n {\"-\"*50} \\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e7dd6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_memory(vector_store, question, chat_history=[]):\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0.1)\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":5})\n",
    "    crc = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
    "    result = crc({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    \n",
    "    return result, chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6470b4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president said that Ketanji Brown Jackson is one of our nation's top legal minds and will continue Justice Breyer's legacy of excellence.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[('What did the president say about Ketanji Brown Jackson?', \"The president said that Ketanji Brown Jackson is one of our nation's top legal minds and will continue Justice Breyer's legacy of excellence.\")]\n"
     ]
    }
   ],
   "source": [
    "chat_history = list()\n",
    "\n",
    "q = 'What did the president say about Ketanji Brown Jackson?'\n",
    "result, chat_history = ask_with_memory(vector_store, q, chat_history)\n",
    "\n",
    "print(result['answer'])\n",
    "print('-' * 100)\n",
    "print(chat_history)  # for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c7263e27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He mentioned that Circuit Court of Appeals Judge Ketanji Brown Jackson will succeed Justice Breyer.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[('What did the president say about Ketanji Brown Jackson?', \"The president said that Ketanji Brown Jackson is one of our nation's top legal minds and will continue Justice Breyer's legacy of excellence.\"), ('Did he mention who she succeeded?', 'He mentioned that Circuit Court of Appeals Judge Ketanji Brown Jackson will succeed Justice Breyer.')]\n"
     ]
    }
   ],
   "source": [
    "q = 'Did he mention who she succeeded?'\n",
    "result, chat_history = ask_with_memory(vector_store, q, chat_history)\n",
    "\n",
    "print(result['answer'])\n",
    "print('-' * 100)\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac03e17",
   "metadata": {},
   "source": [
    "### Ask with Memory Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc8211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "i = 1\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "print(\"Write Quit or Exit to quit\")\n",
    "while True:\n",
    "    q = input(f\"Question #{i}\")\n",
    "    i = i + 1\n",
    "    if q.lower() in [\"quit\",\"exit\"]:\n",
    "        print(\"Qutting\")\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    result, _ = ask_with_memory(vector_store, q, chat_history)\n",
    "    print (result['answer'])\n",
    "    print(\"----------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
